{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World Data Engineering Example\n",
    "\n",
    "## ETL Overview\n",
    "\n",
    "Throughout this lesson, we're going to build a near real world end-to-end _data engineering pipeline_. This lesson will build a multi-stage pipeline that reads, transforms, and writes a JSON row file.\n",
    "\n",
    "The point of this lesson is to familiarize ourselves with common stages of a _Real World Data Pipeline_. A proper data engineering task typically includes the following stages:\n",
    "1. **Extracting** or reading the data from the source\n",
    "2. Adding **Metadata** columns and tags for organizational purposes\n",
    "3. **Data Quality** checks to ensure ingesting clean data\n",
    "4. Data **Transformations** to apply business logic and conform fields to their desired format\n",
    "5. **Loading** the data into the target system (ie: database or the Cloud)\n",
    "\n",
    "This process is typically known as **ETL** short for **E**xtract, **T**ransform, and **L**oad. In short, the term ETL is synonymous with Data Engineering. This is the most common task of a data engineer. **Remember** this term; it's used very often in the industry.\n",
    "\n",
    "![Typical Data Engineering Pipeline](../imgs/de-diagram-01.png)\n",
    "\n",
    "The example in this lesson will cover all the above task. \n",
    "\n",
    "<br/>\n",
    "\n",
    "But before we move on, it's worth noting a few other common stages that are not covered in this lesson...\n",
    "\n",
    "The following steps are commonly associated with _Data Science Pipelines_. A typical data science pipeline might also include the following stages before loading to the target system:\n",
    "1. **Feature Extraction:** Refers to the extraction of key fields/columns (or known as statistical variables) that are specifically used by the machine learning model at hand\n",
    "2. **ML Model Refresh**: Refers to calling the machine learning model algorithm and including its results into our data. A machine learning model typically either predicts an outcome or classify our data with some labels/categories. These are two most common machine learning tasks.\n",
    "\n",
    "As _professional_ data engineers we might also include these steps:\n",
    "1. **Metrics & Volumetrics**: Capturing metrics such as: _processing time, throughput (rows/sec or MBs/sec), load time, number of rows, number of erroneous rows, ..._\n",
    "2. **Logging**: Capturing logs and console logs of our program for troubleshooting later if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Row User Profiles\n",
    "\n",
    "The main purpose of this exercise is to develop a typical data engineering pipeline mimicking the stages explained above. To do this, we're going to build a pipeline that processes a JSON row file containing a series of user profiles. Each row of our file contains a nested JSON user profile. Some of the lines are intentionally tempered with to include errors. Our purpose is to read and transform the \"OK\" (good) rows while writing the error rows into a separate _reject_ file.\n",
    "\n",
    "Let's start by examining our source file. Open the file: [`data/profiles_complex.json`](../data/profiles_complex.json)\n",
    "\n",
    "Each row contains a nested user profile. To help us visualize the nested JSON structure, copy & paste the line into an online JSON formatter tool such as [jsonformatter.org](https://jsonformatter.org/):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"uid\": \"gP2VcwQwgc9oCBUPo7sK88\",\n",
    "  \"name\": \"Sarah Turner\",\n",
    "  \"gender\": \"F\",\n",
    "  \"email\": \"sarah.turner@gmail.com\",\n",
    "  \"birthdate\": \"1924-11-17\",\n",
    "  \"address\": \"58469 Espinoza Crest Apt. 686\\nNew Keithville, CA 65065\",\n",
    "  \"geo_location\": [\n",
    "    \"34.63915\",\n",
    "    \"-120.45794\"\n",
    "  ],\n",
    "  \"credit_cards\": [\n",
    "    {\n",
    "      \"card_type\": \"Maestro\",\n",
    "      \"card_number\": \"6503072302017585\",\n",
    "      \"exp_date\": \"07/24\",\n",
    "      \"cvc\": \"597\"\n",
    "    },\n",
    "    {\n",
    "      \"card_type\": \"Maestro\",\n",
    "      \"card_number\": \"379002902169035\",\n",
    "      \"exp_date\": \"12/29\",\n",
    "      \"cvc\": \"569\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Looking at this JSON structure, we see that:\n",
    "- Each user contains a series of standard fields such: _uid (user id), name, email, gender, ..._\n",
    "- We also have a nested structure for _credit\\_cards_ and _geo\\_location_\n",
    "\n",
    "\n",
    "### Error Rows\n",
    "\n",
    "Take a closer look at the file content. Some of the rows contain invalid data. For example:\n",
    "\n",
    "1. Line #15 and #28 contain _null_ (None) values for user ids (_uid_)\n",
    "2. Line #10 is missing the _email_ field\n",
    "3. A few of the lines like line #30 contain special military addresses\n",
    "\n",
    "Our goal is be to read the file and separate the _\"OK\"_ records from the _\"REJECTS\"_.\n",
    "\n",
    "### Data Pipeline\n",
    "\n",
    "Now, we're going to create a data pipeline to read and transform this file. Our pipelines will include the following stages:\n",
    "\n",
    "- **Stage #0:** Reading the file into JSON rows\n",
    "- **Stage #1:** Adding metadata columns to help us keep things organized\n",
    "- **Stage #2:** A series of data quality checks:\n",
    "   - Schema Check: Check the row schema (structure) to ensure all required fields are present in each row\n",
    "   - Null Check: Check the value of required fields to ensure they are not null or None\n",
    "   - If a row fails to pass either check, it would be _rejected_\n",
    "- **Stage #3:** A series of data transformation tasks:\n",
    "   - Parse Addresses: Break down the address field into four fields: street address, city, state, zip\n",
    "   - Any address non conforming to this standard structure will be _rejected_\n",
    "   - Count the number of credit cards: Add a new field called num_cards which is the count of credit cards records per row\n",
    "- **Stage #4:** Write the output\n",
    "   - Write all _\"OK\"_ (conforming) rows into a separate file\n",
    "   - Write all _rejected_ rows into another file with a brief message on why they were not conforming\n",
    "\n",
    "<br/>\n",
    "\n",
    "To help us visualize this, the following flowchart depicts the logic that we are going to implement:\n",
    "\n",
    "![Pipeline Flow Chart](../imgs/de-diagram-02.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Without any further ado, let's get started...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE #0: Read JSON rows\n",
    "\n",
    "In this stage, we will define a `run()` method to read our JSON file into python dicts using the built-in `json` module. We'll add some additional syntax to account for running into any exceptions. This will be done by using a `try/except` block. Our method will track of number rows processed and print a short line summary at the end.\n",
    "\n",
    "A few things to note about our code:\n",
    "1. We use the long form of `try/except` with an optional `finally` block. The `finally` block always runs at the end of either `try` or `except`. This will ensure that this block of code gets executed regardless if there was an exception or not.\n",
    "2. We keep track of ok rows (`ok_count`), rejected rows (`reject_count`), and total number of rows (`line_num`) using the `try/except/finally` blocks.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGES -------------------\n",
    "#   0) read the file into json rows\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def run(file_name:str, print_lines:bool=False) -> None:\n",
    "    \"\"\"\n",
    "    Reads user profiles from a JSON row formated file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): file path to read\n",
    "        print_lines (bool): print lines to console\n",
    "    \"\"\"\n",
    "    # keep track or row counts\n",
    "    line_num = 1            # current line number\n",
    "    ok_count = 0            # number of rows without errors\n",
    "    reject_count = 0        # number of rows with errors\n",
    "    # open and read the file and read line-by-line\n",
    "    with open(file_name, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                # read json line into a dict\n",
    "                row = json.loads(line.strip())\n",
    "                # print the line\n",
    "                if print_lines:\n",
    "                    print(f\"[{line_num:02d}][OK]: {row}\")\n",
    "                # update line count\n",
    "                ok_count += 1\n",
    "            except Exception as err:\n",
    "                # print error row\n",
    "                print(f\"[{line_num:02d}][ERR]: {str(err)}, [DATA]: {row}\")\n",
    "                reject_count += 1\n",
    "            finally:\n",
    "                # finally block executes regardless if there was an exception or not\n",
    "                # update the line count here\n",
    "                line_num += 1\n",
    "    # print line count summary at the end\n",
    "    print(f\"Read {line_num - 1} rows\")\n",
    "    print(f\"OK rows: {ok_count:02d}, Rejected rows: {reject_count:02d}\")\n",
    "\n",
    "\n",
    "\n",
    "# call our function to test\n",
    "filepath = \"../data/profiles_complex.json\"\n",
    "run(filepath, print_lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE #1: Adding Metadata\n",
    "\n",
    "This is one of the most important stages of **any** data engineering pipeline. Adding metadata fields will help us keep track of data as it makes it through our pipeline. This is typically the very first step in any pipeline. There are some common metadata fields that are always added:\n",
    "\n",
    "1. **`modified_timestamp`:** This is the exact timestamp (in UTC) when we processed this record. In a larger database table, this field will tell which rows are updated recently; making it possible to only query rows within a time range. It's very common that other downstream data pipelines need to access only recently updated rows (vs everything). For example, a downstream pipelines could be a data science workflow to update a machine learning model; or a business intelligence (BI) pipeline to update a report. Any downstream pipelines can query the database table for any rows where _modified\\_timestamp_ is **greater** than their last execution time. This will ensure that they only process newly updated rows. This process is known as **CDC** short for _Change Data Capture_.\n",
    "\n",
    "    It's worth that some engineers also include another _created\\_timestamp_ field. This field corresponds to the initial row creation or insertion into the database vs. _modified\\_timestamp_ is the last modification timestamp.\n",
    "\n",
    "1. **`batch_id`:** This is a unique ID assign to each run of our script (or pipeline). This field links rows to a particular run of our script (with a large database table). Let's say that later on we find a bug in our script or think that somehow the script was compromised. This field will help us to exactly locate the affected rows within a larger database table. Perhaps, we like to delete these rows to reprocess later; we could simply issue a DELETE statement to our database with this batch ID.\n",
    "\n",
    "2. **`tags`:** This is a commonly used field by well-seasoned senior data engineers. This is a nested key/value JSON or dict field. It allows us to store any additional metadata tags regarding this row. Some of the common use-cases are _Row Level Security (RLS)_. This example adds a couple of common _RLS_ fields such a _security\\_level_ and _allowed\\_users_. Let's say that we need to implement a mechanism to easily identify rows containing _Personal Private Information (PPI)_ and allow only certain user groups to query them. These tags will help us to exactly achieve this. We can scan our row for personal information can add the appropriate tags. In North America or European Union this is actually required by law. In this example since our records include name, email, and addresses we will tag everything with the highest level of security.\n",
    "\n",
    "    It's worth mentioning that seasoned data engineers use this field to add more tags. Some common tags are: _processing\\_status_, _source\\_system_, _pipeline\\_name_, etc...\n",
    "\n",
    "<br/>\n",
    "\n",
    "As you gain more experience, you will see that each company likes to define their own set of required _metadata fields_. These fields are also commonly called _ETL_ or _Control_ fields.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Before we move on to the code, please make sure that you **remember** the keywords explained above:\n",
    "- _Metadata_ or _ETL_ or _Control_ fields\n",
    "- _CDC_ (Change Data Capture)\n",
    "- _RLS_ (Row Level Security)\n",
    "- _batch\\_id_, _modified_ and _created_ timestamps\n",
    "\n",
    "<br/>\n",
    "\n",
    "Our `add_metadata()` method will add the following _Metadata_ or _Control_ fields to our rows:\n",
    "- `modified_timestamp`\n",
    "- `batch_id`\n",
    "- `tags` with two RLS fields: `security_level` and `allowed_user_groups`\n",
    "\n",
    "A few things to note about this code:\n",
    "- Our code uses the pypi [`shortuuid`](https://pypi.org/project/shortuuid/) module to create a unique **UUID** (_Universally Unique ID_) for each run of the script. This module contains a method called `uuid()` which returns a unique code every time that is called.\n",
    "- `tags` field is set as a nested key/value dict (or JSON column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGES -------------------\n",
    "#   0) read the file into json rows\n",
    "#   1) add metadata columns\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import shortuuid\n",
    "\n",
    "# get a unique ID for this ETL batch (script)\n",
    "BATCH_ID = shortuuid.uuid()\n",
    "\n",
    "def add_metadata(row:dict) -> None:\n",
    "    \"\"\"\n",
    "    Adds ETL metadata columns to a row. Follwing metadata columns \n",
    "    are added:\n",
    "        - modified_timestamp: current datetime\n",
    "        - batch_id: unique ETL batch_id\n",
    "        - tags: a key/value dict to store various data tags\n",
    "        - tags.errors: a list of error message encountered while processing this row\n",
    "\n",
    "    Args:\n",
    "        row (dict): data row\n",
    "    \"\"\"\n",
    "    global BATCH_ID\n",
    "    now_utc = datetime.utcnow()\n",
    "    # add timestamps for when we have processed this row\n",
    "    row[\"modified_timestamp\"] = now_utc\n",
    "    # add the ETL (script) batch_id\n",
    "    row[\"batch_id\"] = BATCH_ID\n",
    "    # a series of additional processing tags \n",
    "    row[\"tags\"] = {\n",
    "        # other tags can go here, for example:\n",
    "        \"security_level\": \"high\",\n",
    "        \"allow_user_groups\": [\"admin\",]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our `run()` function to call `add_metadata()` and run everything again:\n",
    "\n",
    "> **NOTE:** In order to keep this notebooks slimmer and easier to scroll up/down, we have removed additional code comments here. It's good practice that you always comment your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(file_name:str, print_lines:bool=False) -> None:\n",
    "    \"\"\"\n",
    "    Reads user profiles from a JSON row formated file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): file path to read\n",
    "        print_lines (bool): print lines to console\n",
    "    \"\"\"\n",
    "    # keep track or row counts\n",
    "    line_num = 0            # total number of rows\n",
    "    ok_count = 0            # number of rows without errors\n",
    "    reject_count = 0        # number of rows with errors\n",
    "\n",
    "    with open(file_name, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                row = json.loads(line.strip())\n",
    "                # checks & transformations\n",
    "                add_metadata(row)\n",
    "\n",
    "                if print_lines:\n",
    "                    print(f\"[{line_num:02d}][OK]: {row}\")\n",
    "                ok_count += 1\n",
    "            except Exception as err:\n",
    "                print(f\"[{line_num:02d}][ERR]: {str(err)}, [DATA]: {row}\")\n",
    "                reject_count += 1\n",
    "            finally:\n",
    "                line_num += 1\n",
    "    # print line count summary at the end\n",
    "    print(f\"Read {line_num} rows\")\n",
    "    print(f\"OK rows: {ok_count:02d}, Rejected rows: {reject_count:02d}\")\n",
    "\n",
    "\n",
    "\n",
    "# call our function to test\n",
    "filepath = \"../data/profiles_complex.json\"\n",
    "run(filepath, print_lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE #2: Data Quality Checks\n",
    "\n",
    "As data engineers, it's our job to ensure only clean data enters the system; and that bad or incomplete data is sent back for reprocessing. It's very important that a series of checks are performed in the initial stages, ensuring downstream stages to work only with valid data. This is commonly known as **Data Quality** checks. Common Data Quality checks are:\n",
    "\n",
    "- **Schema Validation:** This process ensures that the row contains all required fields. This is specially important when dealing with JSON rows since JSON's flexible format allows each row to contain different fields. The word **Schema** commonly refers to the field names and data types of the row. \n",
    "\n",
    "- **Data Type Validation:** As part of the Schema check or in a separate step, we also check the field values for correct data types (ie: int, float, string, string length, etc...).\n",
    "\n",
    "- **Null Check:** Ensures that required fields do not contain _Null_ or _None_.\n",
    "\n",
    "- **Referential Integrity Check:** This is a common process when a row references other entities (data tables) in our system (for example containing IDs of other tables such as _customer\\_id_ or _product\\_id_). You've learned that these are fields called _foreign keys_ which refer to _primary keys_ of other tables. It's important to ensure that we do in fact have a matching primary key in other tables. This process is done by performing a _join_ or a **lookup** to other tables. Our example here will NOT include this check but please be aware that this is a very common check.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Before we move onto the code, it's important to **remember** some more keywords from this section:\n",
    "- **Data Quality** the process of checking the quality of data prior to ingestion\n",
    "- **Schema** referring to field names and data types.\n",
    "- **Referential Integrity Check**, the process of checking the existence of foreign keys in their primary source\n",
    "\n",
    "<br/>\n",
    "\n",
    "Our example below will include the following _Data Quality_ methods:\n",
    "1. `schema_check()`: ensure our row conforms to a required schema (including required fields).\n",
    "2. `null_check()`: ensure fields contain valid values and not Nulls or Nones.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Note** a few additional comments regarding our code below:\n",
    "- Any non-conforming rows will raise an exception which will be caught by the `except` block in the `run()` method and rejected properly.\n",
    "- Both `schema_check()` and `null_check()` accept a default set of field names to check. This is done by using two _static_ variables called `REQUIRED_SCHEMA_FIELDS` and `NOT_NULL_FIELDS` in _ALL CAPS_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGES -------------------\n",
    "#   0) read the file into json rows\n",
    "#   1) add metadata columns\n",
    "#   2) Data Quality checks:\n",
    "#       - schema check\n",
    "#       - null check\n",
    "\n",
    "\n",
    "# define a set of required fields (or keys)\n",
    "#   static variable (all caps) used as the default value in `schema_check()` function\n",
    "REQUIRED_SCHEMA_FIELDS = {'uid','name', 'email', 'birthdate', 'credit_cards', 'address', 'gender', 'geo_location', 'modified_timestamp', }\n",
    "\n",
    "def schema_check(row:dict, fields=REQUIRED_SCHEMA_FIELDS) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if all required fields (dict) are present in a dict or JSON row. Any \n",
    "    missing fields will cause a KeyError exception.\n",
    "\n",
    "    Args:\n",
    "        row (dict): data row\n",
    "        fields (set, optional): set of required required fields. Defaults to NOT_NULL_FIELDS.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all fields (keys) are present\n",
    "\n",
    "    Raises:\n",
    "        KeyError: if any of the required fields (keys) are missing in row\n",
    "    \"\"\" \n",
    "    # loop thru the required fields and make sure they are all present\n",
    "    for field in fields:\n",
    "        # if this field is missing in row, raise an exception\n",
    "        if field not in row:\n",
    "            raise KeyError(f\"Missing required field: {field}\")\n",
    "    # return true if no exceptions\n",
    "    return True\n",
    "\n",
    "\n",
    "# default fields to check for null values\n",
    "NOT_NULL_FIELDS = {'uid','name', 'email', 'birthdate', }\n",
    "\n",
    "def null_check(row:dict, fields=NOT_NULL_FIELDS) -> bool:\n",
    "    \"\"\"\n",
    "    Checks the row to NOT contain None values for any of the fields provided. \n",
    "    Any fields containing None would cause a ValueError exception.\n",
    "\n",
    "    Args:\n",
    "        row (dict): data row\n",
    "        fields (set, optional): list of dict keys to check the value for. Defaults to NOT_NULL_FIELDS.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if none of the fields contain None values\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if a field contains None\n",
    "    \"\"\"    \n",
    "    for field in fields:\n",
    "        # check to see if the value of this field is None or null\n",
    "        if row[field] is None:\n",
    "            # add an error message to this row\n",
    "            raise ValueError(f\"{field} can NOT be None or null.\")\n",
    "    # otherwise return True\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the `run()` method to call our **Data Quality** checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(file_name:str, print_lines:bool=False) -> None:\n",
    "    \"\"\"\n",
    "    Reads user profiles from a JSON row formated file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): file path to read\n",
    "        print_lines (bool): print lines to console\n",
    "    \"\"\"\n",
    "    # keep track or row counts\n",
    "    line_num = 0            # total number of rows\n",
    "    ok_count = 0            # number of rows without errors\n",
    "    reject_count = 0        # number of rows with errors\n",
    "\n",
    "    with open(file_name, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                row = json.loads(line.strip())\n",
    "                # checks & transformations\n",
    "                add_metadata(row)\n",
    "                schema_check(row)\n",
    "                null_check(row)\n",
    "                if print_lines:\n",
    "                    print(f\"[{line_num:02d}][OK]: {row}\")\n",
    "                ok_count += 1\n",
    "            except Exception as err:\n",
    "                print(f\"[{line_num:02d}][ERR]: {str(err)}, [DATA]: {row}\")\n",
    "                reject_count += 1\n",
    "            finally:\n",
    "                line_num += 1\n",
    "    # print line count summary at the end\n",
    "    print(f\"Read {line_num} rows\")\n",
    "    print(f\"OK rows: {ok_count:02d}, Rejected rows: {reject_count:02d}\")\n",
    "\n",
    "\n",
    "\n",
    "# call our function to test\n",
    "filepath = \"../data/profiles_complex.json\"\n",
    "run(filepath, print_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE #3: Data Transformations\n",
    "\n",
    "At the end of the day, this is where we spent most of our time as data engineers. In our opinion this is also the most straight-forward and fun step. Data always needs to be transformed and refined before being used by downstream processes. Hence, the term for building a data _pipeline_ referring to the process of refining crude oil and transporting it from one location to another.\n",
    "\n",
    "In our example below, we're going to introduce two transformation methods:\n",
    "1. **`transform_address()`:** This method uses regular expressions (regex) to parse addresses from their standard US multi-line format into corresponding street address, city, state, zip fields. Regular Expressions are a very powerful technique to match string patterns. A brief introduction to regexes is included below. In this method, we use a rather complex regex to break down the address into 4 named groups: street_address, city, state, and zip. We use a regex technique called _Named Capturing Group_ with the syntax of `(?P<group_name>...regex pattern...)`. Pay attention to the four named groups below. Our regex looks for an address broken down into two lines with the first line including the street address, and the second line including the city, state zip separated by a comma a white space respectively.\n",
    "\n",
    "    It's important to note, that our regex will fail to match military addresses. Military address do not include a comma between their city and zip portion, causing our regex pattern not to match. At the moment, these addresses will be rejected. This is a great feature request to improvement our regex to include these addresses (or to create a secondary pattern exclusively for military addresses).\n",
    "\n",
    "2. **`add_num_cards()`:** This is relatively a very simple method which counts the number of credit cards present in the row and adds it to our _schema_.\n",
    "\n",
    "#### Regular Expressions (regex)\n",
    "\n",
    "**Regular Expressions** or **regex** are a very powerful language to match string patterns. They are commonly used to match patterns like phone numbers, emails, urls. For example a valid URL starts with either _http://_ or _https://_, contains _www_ or another sub-domain, a domain name, and ends with _.com_, _.edu_, or _.org_. This is a complex pattern which a Regex can extracts into the following portions: protocol (http or https), domain, sub-domain, and domain type (.com, .net, .edu, ...). Regexes are a far too complex of a topic to cover in this lesson; but we like you to understand their use-case and get familiar with their implementation in Python. Python provides a built-in module called `re` to exploit regular expressions.\n",
    "\n",
    "[Real Python](https://realpython.com/regex-python/) provides an excellent tutorial of regex in Python. We also highly recommend the following tools:\n",
    "- [Regex101](https://regex101.com/): A great tool for building and testing regex patterns online. This page includes a very handy regex _cheatsheet_. Even the most well-seasoned engineers need to use a cheatsheet to refresh their memory with regex syntax. This site export your patterns into code snippets in multiple languages such as Python or Javascript. Making it extremely easy to build, test, and copy/paste the code in your script.\n",
    "- [Regex Gulf](https://alf.nu/RegexGolf?world=regex&level=r00): A great online game for mastering regex patterns.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Ok, let's get back to our transformations... Pay attention to the code below:\n",
    "- We use a complex regex address pattern with _Named Capturing Groups_ to parse addresses\n",
    "- Any address not conforming to our regex pattern will be rejected. At the moment, this will also include military addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGES -------------------\n",
    "#   0) read the file into json rows\n",
    "#   1) add metadata columns\n",
    "#   2) Data Quality checks:\n",
    "#       - schema check\n",
    "#       - null check\n",
    "#   5) transformatios:\n",
    "#       - parse address into street_address, city, state, zip fields\n",
    "#       - add a num_cards field\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def transform_address(row:dict) -> bool:\n",
    "    \"\"\"\n",
    "    Parses the address into street_address, city, state, zip fields. Invalid addresses \n",
    "    cause a ValueError exception.\n",
    "\n",
    "    Args:\n",
    "        row (dict): data row\n",
    "\n",
    "    Returns:\n",
    "        bool: True if address is in valid US address format; otherwise False\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Unknown address format.\n",
    "    \"\"\"\n",
    "    # regular expression (regex) to match a US address composed of street_address, city, state, zip\n",
    "    #   - this regex uses Named Capturing Group feature of regex to assign a name to a matching portion of the string\n",
    "    #   - the syntax is (?<group_name>...) where ... contains the matching regex for this group\n",
    "    address_regex = r\"(?P<street_address>[a-zA-Z0-9 .]+)\\n(?P<city>[a-zA-Z0-9 ]+), (?P<state>[A-Z]{2}) (?P<zip>[0-9]{5})\"\n",
    "    pattern = re.compile(address_regex)\n",
    "    # match address using regex\n",
    "    result = pattern.match(row[\"address\"])\n",
    "    if result:\n",
    "        # if a possible match found. assign fields based on regex matching named groups\n",
    "        row[\"street_address\"] = result.group(\"street_address\")\n",
    "        row[\"city\"] = result.group(\"city\")\n",
    "        row[\"state\"] = result.group(\"state\")\n",
    "        row[\"zip\"] = result.group(\"zip\")\n",
    "        # delete the original address field\n",
    "        del row[\"address\"]\n",
    "        return True\n",
    "    else:\n",
    "        # if address is not a match, raise an exception\n",
    "        address = row[\"address\"].strip().replace('\\n', ', ')        # remove ENTER from the address\n",
    "        raise ValueError(f\"Unknown address format: {address}\")\n",
    "\n",
    "\n",
    "def add_num_cards(row:dict) -> None:\n",
    "    \"\"\"\n",
    "    Adds a field call num_cards with the total number of credit cards in this row.\n",
    "\n",
    "    Args:\n",
    "        row (dict): data row.\n",
    "    \"\"\"\n",
    "    if \"credit_cards\" in row and isinstance(row[\"credit_cards\"], list):\n",
    "        row[\"num_cards\"] = len(row[\"credit_cards\"])\n",
    "    else:\n",
    "        row[\"num_cards\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the `run()` method again for call our data transformation methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(file_name:str, print_lines:bool=False) -> None:\n",
    "    \"\"\"\n",
    "    Reads user profiles from a JSON row formated file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): file path to read\n",
    "        print_lines (bool): print lines to console\n",
    "    \"\"\"\n",
    "    # keep track or row counts\n",
    "    line_num = 0            # total number of rows\n",
    "    ok_count = 0            # number of rows without errors\n",
    "    reject_count = 0        # number of rows with errors\n",
    "\n",
    "    with open(file_name, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                row = json.loads(line.strip())\n",
    "                # checks & transformations\n",
    "                add_metadata(row)\n",
    "                schema_check(row)\n",
    "                null_check(row)\n",
    "                transform_address(row)\n",
    "                add_num_cards(row)\n",
    "                if print_lines:\n",
    "                    print(f\"[{line_num:02d}][OK]: {row}\")\n",
    "                ok_count += 1\n",
    "            except Exception as err:\n",
    "                print(f\"[{line_num:02d}][ERR]: {str(err)}, [DATA]: {row}\")\n",
    "                reject_count += 1\n",
    "            finally:\n",
    "                line_num += 1\n",
    "    # print line count summary at the end\n",
    "    print(f\"Read {line_num} rows\")\n",
    "    print(f\"OK rows: {ok_count:02d}, Rejected rows: {reject_count:02d}\")\n",
    "\n",
    "\n",
    "\n",
    "# call our function to test\n",
    "filepath = \"../data/profiles_complex.json\"\n",
    "run(filepath, print_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE #4: Final Preparation\n",
    "\n",
    "By now, our rows are transformed into the correct structure and any bad rows are rejected. Let's modify our `run()` method to create two separate file to contain our _OK_ and _rejected_ rows. It's very common to insert the _OK_ records into a database table while sending back the rejected rows for reprocessing or a manual intervention.\n",
    "\n",
    "Pay attention to the code below:\n",
    "- We compose two new file names including the original name, a timestamp, and the postfix of either _ok_ or _rejected_. These file names are stored inside `ok_file_name` and `reject_file_name` variables.\n",
    "- We open two file handle (objects) respectively with the names of `ok_file` and `reject_file`. Since these handles are _NOT_ opened using the `with` statement, we _must_ remember to close them at the end using their `.close()` methods.\n",
    "- These files are written using `json.dump()` within the `try` and `except` blocks.\n",
    "- The `except` block capture the exception raised during our processing as a variable called `err`. This error message is added to the row as a key called `row[\"error\"]`. This will make it easy for anyone reprocessing the file to know why exactly the row was rejected.\n",
    "- Since our JSON structure includes `datetime` timestamps, we need to provide a special `JSONEncoder` to format them into a datetime string. This is done by inheriting the default `JSONEncoder` class into a `DatetimeEncoder` class. This class overrides the `default()` method to specifically identify and format datetime fields. **Note** that the default `JSONEncoder` class can only encode `int`, `str`, `float`, `list`, and `dict` types. If we need to write any other data type, we need to provide our own encoder class.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Ok, let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from json import JSONEncoder\n",
    "\n",
    "\n",
    "class DatetimeEncoder(JSONEncoder):\n",
    "    \"\"\"\n",
    "    Custom JSON Encoder class to properly encode datetime fields. All other fields are encoded with \n",
    "    their default formatting. This class inherits the default json.JSONEncoder class.\n",
    "    \"\"\"\n",
    "\n",
    "    def default(self, value):\n",
    "        \"\"\"Encodes values to JSON. Adds special formatting for datetime fields.\n",
    "\n",
    "        Args:\n",
    "            value (object): field to encode\n",
    "\n",
    "        Returns:\n",
    "            object: encoded json string\n",
    "        \"\"\"\n",
    "        # check for datetime type\n",
    "        if isinstance(value, datetime):\n",
    "            # format the datetime string\n",
    "            dtfmt = \"%Y-%d-%m %H:%M:%S.%f %z\"\n",
    "            return datetime.strftime(value, dtfmt)\n",
    "        # all other data types default to the JSONEncoder parent class formatting\n",
    "        super(DatetimeEncoder, self).default(value)\n",
    "\n",
    "\n",
    "def run(file_name:str, print_lines:bool=False) -> None:\n",
    "    \"\"\"\n",
    "    Reads user profiles from a JSON row formated file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): file path to read\n",
    "        print_lines (bool): print lines to console\n",
    "    \"\"\"\n",
    "    # keep track or row counts\n",
    "    line_num = 0            # total number of rows\n",
    "    ok_count = 0            # number of rows without errors\n",
    "    reject_count = 0        # number of rows with errors\n",
    "\n",
    "    # prepare a ok & reject file\n",
    "    # -------------------------------------\n",
    "    # add a timestamp to files\n",
    "    file_timestamp = datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "    # get the file name without it's extension\n",
    "    file_name_without_extension = file_name.rpartition('.')[0]      # from the right of the string, partition by '.' and take the first partition\n",
    "    # create ok and reject file names inclusing the timestamp\n",
    "    ok_file_name = f\"{file_name_without_extension}_{file_timestamp}_ok.json\"\n",
    "    reject_file_name = f\"{file_name_without_extension}_{file_timestamp}_reject.json\"\n",
    "    # open files for writing\n",
    "    ok_file = open(ok_file_name, \"w\", encoding=\"utf-8\")\n",
    "    reject_file = open(reject_file_name, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    with open(file_name, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                row = json.loads(line.strip())\n",
    "                # checks & transformations\n",
    "                add_metadata(row)\n",
    "                schema_check(row)\n",
    "                null_check(row)\n",
    "                transform_address(row)\n",
    "                add_num_cards(row)\n",
    "                # write to ok file\n",
    "                json.dump(row, ok_file, cls=DatetimeEncoder)     # write the json row\n",
    "                ok_file.write(\"\\n\")         # write endline character\n",
    "\n",
    "                if print_lines:\n",
    "                    print(f\"[{line_num:02d}][OK]: {row}\")\n",
    "                ok_count += 1\n",
    "            except Exception as err:\n",
    "                # add error and line number to the json row\n",
    "                err_msg = f\"[{line_num:02d}][ERR]: {str(err)}\"\n",
    "                row[\"error\"] = err_msg\n",
    "                # write the error line to reject file\n",
    "                json.dump(row, reject_file, cls=DatetimeEncoder)\n",
    "                reject_file.write('\\n')\n",
    "                print(err_msg, row)\n",
    "                reject_count += 1\n",
    "            finally:\n",
    "                line_num += 1\n",
    "    # print line count summary at the end\n",
    "    print(f\"Read {line_num} rows\")\n",
    "    print(f\"OK rows: {ok_count:02d}, Rejected rows: {reject_count:02d}\")\n",
    "    # close files\n",
    "    ok_file.close()\n",
    "    reject_file.close()\n",
    "\n",
    "\n",
    "\n",
    "# call our function to test\n",
    "filepath = \"../data/profiles_complex.json\"\n",
    "run(filepath, print_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Pulling it all together\n",
    "\n",
    "Stitching all the stages above into a final comprehensive Python _module_ under: `src/process_profiles.py`\n",
    "\n",
    "This module accepts two command line arguments using the built-in `sys.args` module. These are the two parameters into our `run()` method. The first argument reflects the file_name to process; while the second argument indicates to print the lines to console or not. This module calls our `run()` method passing these parameters. Examine the module code; once you're satisfied, execute via the terminal:\n",
    "\n",
    "```bash\n",
    "cd src/\n",
    "python3.7 process_profiles.py \"../data/profiles_complex.json\" yes\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ad1438936aa7bcf10cf84ad983120386fbdfa17afaa8d0ccc426c556e78c086"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Overview\n",
    "\n",
    "In addition to CSVs, JSON is another popular format for transporting data. This format is popular amongst web and mobile applications and it stand for JavaScript Object Annotation (JSON). JSON provides a very flexible structure for representing data. Another benefit of JSON is that it provides the means for having complex nested and hierarchal data structures. This is in contrast to CSVs where they only rigid and flat data columnar structures. \n",
    "\n",
    "Let's see an example a JSON object:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"Freddie Mercury\",\n",
    "    \"born\": \"Farrokh Bulsara\",\n",
    "    \"nationality\": \"British\",\n",
    "    \"occupation\": \"Singer, Song Writer\",\n",
    "    \"age\": 45\n",
    "}\n",
    "```\n",
    "\n",
    "JSON objects are very similar to a Python `dict`. In this example, our JSON object contains the following keys: _name, born, nationality, occupation, and age_. Each key as its value followed by a semicolon. A field value could include ints, floats, strings, booleans, or other complex structures like lists or other JSON objects (dicts). There are a few exceptions to JSON objects and Python dicts:\n",
    "- A JSON object can start as an array of other JSONs such as: `[{first_json}, {second_json}, ...]`\n",
    "- JSON key names must always be enclosed in double quotes `\"` and not single quotes `'`\n",
    "- JSON None value is defined by the keyword `null` such as `{\"name\": null}`\n",
    "\n",
    "While flexibility and the hierarchal nature of JSON are two of its main advantages, JSON files have some disadvantages:\n",
    "- They are relatively large since each row includes both field names and their values. This can get very redundant and results in large file sizes; making it not very ideal for transporting very large data volumes.\n",
    "- Because of their relatively large size and complexity, JSON files take longer execution time and more processing power to parse. They are typically amongst the slowest file types. Although modern Big Data platforms have improved their processing time via distributed algorithms.\n",
    "\n",
    "This lesson will cover how to read/write json objects in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading JSON\n",
    "\n",
    "Python provides a built-in **`json`** module to read/write JSON objects into Python `dict`. This module makes it extremely easy to convert back and forth from Python dicts and JSON objects.\n",
    "\n",
    "Let's look at the following file as an example: [`data/freddie.json`](../data/freddie.json)\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"Freddie Mercury\",\n",
    "    \"born\": \"Farrokh Bulsara\",\n",
    "    \"birth_date\": \"5 September 1946\",\n",
    "    \"nationality\": \"British\",\n",
    "    \"occupation\": \"Singer, Song Writer\",\n",
    "    \"age\": 45,\n",
    "    \"status\": \"Legend\",\n",
    "    \"biography\": {\n",
    "        \"early_life\": \"Mercury was born Farrokh Bulsara in Stone Town in the British protectorate of Zanzibar.\",\n",
    "        \"vocals\": \"Although Mercury's speaking voice naturally fell in the baritone range, he delivered most songs in the tenor range.\",\n",
    "        \"song_writing\": \"Mercury wrote 10 of the 17 songs on Queen's Greatest Hits album: \\\"Bohemian Rhapsody\\\", \\\"Killer Queen\\\", ...\",\n",
    "        \"relationships\": \"In the early 1970s, Mercury had a long-term relationship with Mary Austin. By 1985, he began another long-term relationship with Irish-born hairdresser Jim Hutton\"\n",
    "    },\n",
    "    \"tributes\": [\n",
    "        \"Statue of Freddie Mercury overlooking Lake Geneva in Montreux, Switzerland\",\n",
    "        \"Mercury statue above the West End's Dominion Theatre\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Pay special attention to the hierarchal nature of this JSON object:\n",
    "- The _biography_ field is itself another JSON object containing keys and values (_early\\_life, vocals, song\\_writing, ..._)\n",
    "- The _tributes_ field is a nested list of strings\n",
    "\n",
    "<br/>\n",
    "\n",
    "Now, let's look at the built-in `json` module in action to read this file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the built-in json module\n",
    "import json\n",
    "\n",
    "# open file for reading\n",
    "with open(\"../data/freddie.json\", \"r\") as json_file:\n",
    "    # load the json into a dict\n",
    "    freddie_dict = json.load(json_file)\n",
    "\n",
    "    # example: access json felids\n",
    "    #   simply access dict/json fields by key names\n",
    "    print(freddie_dict[\"name\"])\n",
    "\n",
    "    # example: loop thru complex felids\n",
    "    #   print tributes\n",
    "    if \"tributes\" in freddie_dict:                  # check to see if key exists\n",
    "        for tribute in freddie_dict[\"tributes\"]:    # loop thru list\n",
    "            print(tribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dissect this code:\n",
    "- The `json` module provides a `.load()` method to read the entire content of a JSON file.\n",
    "- This method return a Python `dict` object. We can easily access the JSON fields via the dict keys. For example: `freddie[\"name\"]` will access the name of Queen's legendary vocalist and song writer, Freddie Mercury.\n",
    "- This method takes an open file handle for reading. We open the file using the familiar built-in `open()` method.\n",
    "- Pay special attention that the `.load()` method expects the JSON file to contain a valid (and single) json record enclosed in brackets `{}`. This method raises an exception if there are any issues in the formatting of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing JSON\n",
    "\n",
    "The `json.load()` method is accompanied by its reversed `json.dump()` method which writes a Python `dict` into a JSON file. This method takes two positional parameters. The first parameter is the Python dict to write into the file while the second parameter is the open file handle for writing. \n",
    "\n",
    "Let's see this method in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# let's write freddie's dict back to a file\n",
    "print(freddie_dict)\n",
    "# let's add a field\n",
    "freddie_dict[\"to\"] = \"we love you eternally\"\n",
    "\n",
    "# open a file for writing\n",
    "with open(\"../data/freddie_2.json\", \"w\") as json_file:\n",
    "    # write a dictionary as json\n",
    "    #   pay attention: this line is printed in compressed format with no\n",
    "    #   no line separators or indention\n",
    "    json.dump(freddie_dict, json_file)\n",
    "\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the code above:\n",
    "- We add a new key to our freddie dict\n",
    "- We open a file for writing via the familiar `open()` method\n",
    "- The `json.dump()` method write the content of our dict into a file\n",
    "- By default, this method writes a compressed JSON format skipping indentation formatting and adding newlines\n",
    "\n",
    "Open _data/freddie\\_2.json_ file and inspect its content.\n",
    "\n",
    "<br/>\n",
    "\n",
    "It's important to note some **limitations** of the `json.dump()` method. This method expects the values of the dict fields to be _JSON Serializable_ which means that it can only write the following data types: `int`, `float`, `str`, `boolean`, `list`, and `dict`. Other more complex types such as `datetime` will raise an exception. Later we will see how to write our own JSON encoders to _serialize_ complex types (ie: format them into string).\n",
    "\n",
    "To avoid this exception for now, we're going to add two optional parameters to our `.dump()` method:\n",
    "- `skipkeys=True` tells the method to skip any complex fields and only serialize (write) simple ones\n",
    "- `indent=4` formats the JSON output to be easier to read\n",
    "\n",
    "Let's run the code again and compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# let's write freddie's dict back to a file\n",
    "\n",
    "# let's add a field\n",
    "freddie_dict[\"to\"] = \"we love you eternally\"\n",
    "print(freddie_dict)\n",
    "\n",
    "# open a file for writing\n",
    "with open(\"../data/freddie_2.json\", \"w\") as json_file:\n",
    "    # write a dictionary as json\n",
    "    #   pay attention: this line is printed in compressed format with no\n",
    "    #   no line separators or indention\n",
    "    json.dump(freddie_dict, json_file, indent=4, skipkeys=True)\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read/Write JSON Strings\n",
    "\n",
    "In addition to reading and writing to files, the `json` modules provides two additional methods called `loads()` and `dumps()` that work with strings (instead of files). The `loads()` method loads JSON string into a `dict` while the `dumps()` method reversibly writes a `dict` into a string.\n",
    "\n",
    "Let's examine these methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_string = \"\"\"\n",
    "    {\n",
    "        \"name\": \"Freddie Mercury\",\n",
    "        \"born\": \"Farrokh Bulsara\",\n",
    "        \"nationality\": \"British\",\n",
    "        \"occupation\": \"Singer, Song Writer\",\n",
    "        \"age\": 45\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "# read json string to dict\n",
    "freddie = json.loads(json_string)\n",
    "print(freddie)\n",
    "print(type(freddie))\n",
    "\n",
    "# write dict back to json string\n",
    "freddie[\"genres\"] = \"Rock\"\n",
    "json_string = json.dumps(freddie)\n",
    "print(\"json:\", json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "The the following dictionary to a json string then read the json string back into a `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sir_elton = {\n",
    "    \"name\": \"Sir Elton Hercules John\",\n",
    "    \"born\": \"Reginald Kenneth Dwight\",\n",
    "    \"occupation\": \"Singer, pianist, composer\",\n",
    "    \"instruments\": \"Vocals, piano, keyboards\",\n",
    "    \"age\": 75\n",
    "}\n",
    "\n",
    "# write the dict into a JSON string\n",
    "\n",
    "# write the dict into a JSON **file** now\n",
    "\n",
    "# read the JSON string back into dict\n",
    "\n",
    "# read the json file into a dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with JSON Row Files\n",
    "\n",
    "JSON Row is a special file format which each line of the file contains a separate JSON object. This file stores a large of number of _json rows_ as individual lines.\n",
    "\n",
    "You can see an example of this in: `data/profiles_simple.json`\n",
    "\n",
    "This file contains multiple lines. Each line contains a JSON object storing a user profile. To help visualize the JSON structure, copy/paste the first line into an online JSON formatter such as [jsonformatter.org](http://jsonformatter.org):\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"uid\":\"h5jYqxqAHAQEhCx2rbuSZJ\",\n",
    "   \"name\":\"Tara White\",\n",
    "   \"gender\":\"F\",\n",
    "   \"email\":\"tara.white@gmail.com\",\n",
    "   \"birthdate\":\"1963-08-23\",\n",
    "   \"salary\":143720.55,\n",
    "   \"credit_score\":511,\n",
    "   \"active\":true\n",
    "}\n",
    "```\n",
    "\n",
    "The familiar `json.loads()` method makes reading this file format very easy. We can:\n",
    "- Open the file for reading\n",
    "- Read the file content line by line\n",
    "- And call the `json.loads()` method to covert the JSON object into dicts\n",
    "\n",
    "Let's see this in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# hold all profiles\n",
    "profiles = []\n",
    "\n",
    "# open the file for reading\n",
    "with open(\"../data/profiles_simple.json\", \"r\") as json_file:\n",
    "    line_num = 1\n",
    "    for line in json_file:\n",
    "        # read json string (the line) into a dict\n",
    "        row = json.loads(line.strip())\n",
    "        # print the dict\n",
    "        print(f\"{line_num:02d}: {row}\")\n",
    "        # incr. line number & append profiles\n",
    "        line_num += 1\n",
    "        profiles.append(row)\n",
    "\n",
    "print(f\"Read {len(profiles)} profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Schema and Data Types\n",
    "\n",
    "Since JSON objects are highly flexible and can have varying fields between different rows, it's common data engineering practice to always check their structure (schema) and data types upon ingestion. This will ensure that the JSON row contains all the required fields and decode any special types (like datetime) into their appropriate Python types.\n",
    "\n",
    "The code below introduces two new functions for reading our profiles:\n",
    "1. `check_schema()`: This method check the JSON row (dict) for a series of required keys. This function return False if any keys are missing. The list of keys to check are passed as a static set called `REQUIRED_SCHEMA_FIELDS`.\n",
    "2. `parse_date()`: Parses the _birthdate_ field into a python datetime object using the `datetime.strptime()` method.\n",
    "\n",
    "Let's see this action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# hold all profiles\n",
    "profiles = []\n",
    "\n",
    "\n",
    "# set of required fields in the json schema\n",
    "REQUIRED_SCHEMA_FIELDS = {'uid', 'name', 'email', 'birthdate'}\n",
    "\n",
    "def check_schema(row, required_fields=REQUIRED_SCHEMA_FIELDS):\n",
    "    \"\"\"\n",
    "    checks if a json row (or dict) contains all required fields (or keys)\n",
    "    \"\"\"\n",
    "    # loop through all the required fields\n",
    "    for field_name in required_fields:\n",
    "        # return false if any key is not in the dict\n",
    "        if field_name not in row:\n",
    "            return False\n",
    "    # otherwise return true\n",
    "    return True\n",
    "\n",
    "\n",
    "def parse_date(value, dtfmt=\"%Y-%m-%d\"):\n",
    "    \"\"\"\n",
    "    function to parse a date string into datetime.date object.\n",
    "    return None if there are any issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return dt.strptime(value, dtfmt).date()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# open the file for reading\n",
    "with open(\"../data/profiles_simple.json\", \"r\") as json_file:\n",
    "    line_num = 1\n",
    "    for line in json_file:\n",
    "        # read json string (the line) into a dict\n",
    "        row = json.loads(line.strip())\n",
    "        # check json schema\n",
    "        if not check_schema(row):\n",
    "            # print error message if the row schema is incorrect\n",
    "            msg = f\"Invalid Row Schema (missing required fields): {row}\"\n",
    "            print(msg)\n",
    "            # optionally we could raise an exception\n",
    "            # raise ValueError(msg)\n",
    "        else:\n",
    "            # parse fields\n",
    "            row[\"birthdate\"] = parse_date(row[\"birthdate\"])\n",
    "            # print the dict\n",
    "            print(f\"{line_num:02d}: {row}\")\n",
    "            profiles.append(row)\n",
    "        # incr. line number\n",
    "        line_num += 1\n",
    "\n",
    "print(f\"Read {len(profiles)} profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code you can see that all _birthdate_ fields are now converted into python `datetime`. \n",
    "\n",
    "Try editing the file, remove some required fields, and run the block again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vehicles Exercise\n",
    "\n",
    "- Read the JSON Row file `data/vehicles_simple.json`\n",
    "- Convert each row into a `dict`\n",
    "- Create a schema check function to validate the following fields are included in each row: _license\\_plate, make\\_model, year, registered\\_date, and registered\\_name_\n",
    "- Create a method to convert the _registered\\_date_ field into a `date` object\n",
    "- Create another method to separates the _make\\_model_ field into two individual _make_ and _model_ fields. Hint: these values are separated by a comma (,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema check and transformation methods\n",
    "\n",
    "# read vehicles_simple.json file row by row\n",
    "#   apply your checks & transformations\n",
    "#   print the rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing Special Data Types via a Custom JSON Encoder\n",
    "\n",
    "As mentioned above, the default `JSONEncoder` class used by the `dump()` and `dumps()` methods is **only** capable of _basic types:_  str, int, float, bool, None, list, dict. A `TypeError` is raised by these methods if we try to write any other data type such as datetime.\n",
    "\n",
    "Try running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "david_gilmour = {\n",
    "    \"name\": \"David Gimour\",\n",
    "    \"birthdate\": datetime(1946, 3, 6),\n",
    "}\n",
    "\n",
    "json_string = json.dumps(david_gilmour)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to resolve this, we must provide our own json encoder class which is capable of serializing (converting to string) any special fields. We do this by inheriting the default `json.JSONEcoder` class and overriding its `default()` method. This method is used by `json.dump()` and `json.dumps()` methods to serialize json objects. \n",
    "\n",
    "The code below:\n",
    "- Create a custom json encoder class called `SimpleJSONEncoder`\n",
    "- This class inherits the default `JSONEncoder` and overrides its `default()` method\n",
    "- The `default()` method check for `date` data types and converts them properly to a date string using `datetime.strftime()` method\n",
    "- Any other data types is passed back to the default parent class encoder (JSONEncoder class) using the `super()` method\n",
    "- We use our custom json encoder while calling the `dumps()` method by setting its `cls=` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from json import JSONEncoder\n",
    "\n",
    "class SimpleJSONEncoder(JSONEncoder):\n",
    "\n",
    "    def default(self, value):\n",
    "        # check if the field is a date object\n",
    "        if isinstance(value, date):\n",
    "            return datetime.strftime(value, \"%Y-%m-%d\")\n",
    "        elif isinstance(value, datetime):\n",
    "            return datetime.strftime(value, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        else:\n",
    "            return super(SimpleJSONEncoder, self).default(value)\n",
    "\n",
    "\n",
    "david_gilmour = {\n",
    "    \"name\": \"David Gimour\",\n",
    "    \"birthdate\": dt(1946, 3, 6),\n",
    "}\n",
    "\n",
    "# use our custom JSONEncoder\n",
    "json_string = json.dumps(david_gilmour, cls=SimpleJSONEncoder)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stackoverflow is always a great place to search code snippets for special json encoders when needed 😉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- Read the vehicle records that you parsed back into a JSON row formatted file\n",
    "- Remember that you parsed your _registered\\_date_ fields are python date objects\n",
    "- You must create a custom json encoder to serialize these fields back into string\n",
    "- Modify your encoder to also serialize int types as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom json encoder to serialize date and int classes\n",
    "\n",
    "# write your vehicle records into a json row file\n",
    "# use your custom encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations, you can now effectively work with JSON file formats. They are a very flexible and powerful method to transport data. Most modern databases such as MySQL and Google BigQuery provide special data types to store and query JSON fields. These databases provide special JSON functions to work with these fields. These fields are **very powerful** since they bring the _modern, complex, flexible, nested_ structure of JSON into the flat and rigid structure of database. \n",
    "\n",
    "Feel free to refer to [MySQL](https://dev.mysql.com/doc/refman/5.7/en/json-function-reference.html) and [Google BigQuery](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions) documentation on their JSON support. Google BigQuery can also natively ingest data from a JSON Row formatted file; making it ideal to directly ingest data from web and mobile applications.\n",
    "\n",
    "Further, most Non-SQL databases such as MongoDB, Google Firestore, and Google Firebase store rows directly in JSON format. This allows web and mobile applications (the primary users of these databases) to directly store their Javascript objects into databases. The flexible and nested structure of JSON enables applications to easily add/drop fields as new versions of applications role out without having to modify the database tables structure for existing rows. Remember that adding (or dropping) a field in SQL or Relational databases require us to modify the entire table definition possibly causing issues.\n",
    "\n",
    "While JSON is a very commonly used data format amongst web and mobile application, an enhanced format called **parquet** is a very common in Cloud and Big Data applications. We will learn about this format later. For now, know that this format provides the flexibility of JSON while addressing its limitations on being large and slow to process. Parquet format is well supported by Pandas and Big Data platforms such as Google BigQuery and Spark.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ad1438936aa7bcf10cf84ad983120386fbdfa17afaa8d0ccc426c556e78c086"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
